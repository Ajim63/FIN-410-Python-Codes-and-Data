{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Stock Market Data\n",
    "\n",
    "\n",
    "We will begin by examining some numerical and graphical summaries of the Smarket data, which is part of the ISLR library. This data set consists of percentage returns for the S&P 500 stock index over 1, 250 days, from the beginning of 2001 until the end of 2005. For each date, we have recorded the percentage returns for each of the five previous trading days, Lag1 through Lag5. We have also recorded Volume (the number of shares traded on the previous day, in billions), Today (the percentage return on the date in question) and Direction (whether the market was Up or Down on this date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.1 The Stock Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Smarket = pd.read_table(\"Data/Smarket.csv\", sep = ',')\n",
    "Smarket = Smarket = Smarket.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250, 9) \n",
      "\n",
      "              Year         Lag1         Lag2         Lag3         Lag4  \\\n",
      "count  1250.000000  1250.000000  1250.000000  1250.000000  1250.000000   \n",
      "mean   2003.016000     0.003834     0.003919     0.001716     0.001636   \n",
      "std       1.409018     1.136299     1.136280     1.138703     1.138774   \n",
      "min    2001.000000    -4.922000    -4.922000    -4.922000    -4.922000   \n",
      "25%    2002.000000    -0.639500    -0.639500    -0.640000    -0.640000   \n",
      "50%    2003.000000     0.039000     0.039000     0.038500     0.038500   \n",
      "75%    2004.000000     0.596750     0.596750     0.596750     0.596750   \n",
      "max    2005.000000     5.733000     5.733000     5.733000     5.733000   \n",
      "\n",
      "             Lag5       Volume        Today  \n",
      "count  1250.00000  1250.000000  1250.000000  \n",
      "mean      0.00561     1.478305     0.003138  \n",
      "std       1.14755     0.360357     1.136334  \n",
      "min      -4.92200     0.356070    -4.922000  \n",
      "25%      -0.64000     1.257400    -0.639500  \n",
      "50%       0.03850     1.422950     0.038500  \n",
      "75%       0.59700     1.641675     0.596750  \n",
      "max       5.73300     3.152470     5.733000  \n"
     ]
    }
   ],
   "source": [
    "print(Smarket.shape, \"\\n\")\n",
    "print(Smarket.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>5.010</td>\n",
       "      <td>1.1913</td>\n",
       "      <td>0.959</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>1.2965</td>\n",
       "      <td>1.032</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>1.4112</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>1.2760</td>\n",
       "      <td>0.614</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.614</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>1.2057</td>\n",
       "      <td>0.213</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year   Lag1   Lag2   Lag3   Lag4   Lag5  Volume  Today Direction\n",
       "0  2001  0.381 -0.192 -2.624 -1.055  5.010  1.1913  0.959        Up\n",
       "1  2001  0.959  0.381 -0.192 -2.624 -1.055  1.2965  1.032        Up\n",
       "2  2001  1.032  0.959  0.381 -0.192 -2.624  1.4112 -0.623      Down\n",
       "3  2001 -0.623  1.032  0.959  0.381 -0.192  1.2760  0.614        Up\n",
       "4  2001  0.614 -0.623  1.032  0.959  0.381  1.2057  0.213        Up"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Smarket.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corr() function produces a matrix that contains all of the pairwise correlations among the predictors in a data set. The first command below gives an error message because the Direction variable is qualitative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Year      Lag1      Lag2      Lag3      Lag4      Lag5    Volume  \\\n",
      "Year    1.000000  0.029700  0.030596  0.033195  0.035689  0.029788  0.539006   \n",
      "Lag1    0.029700  1.000000 -0.026294 -0.010803 -0.002986 -0.005675  0.040910   \n",
      "Lag2    0.030596 -0.026294  1.000000 -0.025897 -0.010854 -0.003558 -0.043383   \n",
      "Lag3    0.033195 -0.010803 -0.025897  1.000000 -0.024051 -0.018808 -0.041824   \n",
      "Lag4    0.035689 -0.002986 -0.010854 -0.024051  1.000000 -0.027084 -0.048414   \n",
      "Lag5    0.029788 -0.005675 -0.003558 -0.018808 -0.027084  1.000000 -0.022002   \n",
      "Volume  0.539006  0.040910 -0.043383 -0.041824 -0.048414 -0.022002  1.000000   \n",
      "Today   0.030095 -0.026155 -0.010250 -0.002448 -0.006900 -0.034860  0.014592   \n",
      "\n",
      "           Today  \n",
      "Year    0.030095  \n",
      "Lag1   -0.026155  \n",
      "Lag2   -0.010250  \n",
      "Lag3   -0.002448  \n",
      "Lag4   -0.006900  \n",
      "Lag5   -0.034860  \n",
      "Volume  0.014592  \n",
      "Today   1.000000  \n"
     ]
    }
   ],
   "source": [
    "print(Smarket.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Year      Lag1      Lag2      Lag3      Lag4      Lag5    Volume  \\\n",
      "Year    1.000000  0.029700  0.030596  0.033195  0.035689  0.029788  0.539006   \n",
      "Lag1    0.029700  1.000000 -0.026294 -0.010803 -0.002986 -0.005675  0.040910   \n",
      "Lag2    0.030596 -0.026294  1.000000 -0.025897 -0.010854 -0.003558 -0.043383   \n",
      "Lag3    0.033195 -0.010803 -0.025897  1.000000 -0.024051 -0.018808 -0.041824   \n",
      "Lag4    0.035689 -0.002986 -0.010854 -0.024051  1.000000 -0.027084 -0.048414   \n",
      "Lag5    0.029788 -0.005675 -0.003558 -0.018808 -0.027084  1.000000 -0.022002   \n",
      "Volume  0.539006  0.040910 -0.043383 -0.041824 -0.048414 -0.022002  1.000000   \n",
      "Today   0.030095 -0.026155 -0.010250 -0.002448 -0.006900 -0.034860  0.014592   \n",
      "\n",
      "           Today  \n",
      "Year    0.030095  \n",
      "Lag1   -0.026155  \n",
      "Lag2   -0.010250  \n",
      "Lag3   -0.002448  \n",
      "Lag4   -0.006900  \n",
      "Lag5   -0.034860  \n",
      "Volume  0.014592  \n",
      "Today   1.000000  \n"
     ]
    }
   ],
   "source": [
    "variables = Smarket.columns.tolist()\n",
    "print(Smarket.drop(variables[8], axis = 1).corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one would expect, the correlations between the lag variables and to- day’s returns are close to zero. In other words, there appears to be little correlation between today’s returns and previous days’ returns. The only substantial correlation is between Year and Volume. By plotting the data we see that Volume is increasing over time. In other words, the average number of shares traded daily increased from 2001 to 2005.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(Smarket)\n",
    "x = range(n)\n",
    "plt.scatter(x, Smarket.Volume, s = 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.2 Logistic Regression \n",
    "Next, we will fit a logistic regression model in order to predict Direction using Lag1 through Lag5 and Volume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Smarket\n",
    "X = data.iloc[:,1:8].copy()\n",
    "y = data.iloc[:,-1].copy()\n",
    "n = len(y)\n",
    "for i in range(n):\n",
    "    if y[i] == 'Up':\n",
    "        y[i] = 1\n",
    "    elif y[i] == 'Down':\n",
    "        y[i] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>5.010</td>\n",
       "      <td>1.1913</td>\n",
       "      <td>0.959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>1.2965</td>\n",
       "      <td>1.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>1.4112</td>\n",
       "      <td>-0.623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>1.2760</td>\n",
       "      <td>0.614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.614</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>1.2057</td>\n",
       "      <td>0.213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Lag1   Lag2   Lag3   Lag4   Lag5  Volume  Today\n",
       "0  0.381 -0.192 -2.624 -1.055  5.010  1.1913  0.959\n",
       "1  0.959  0.381 -0.192 -2.624 -1.055  1.2965  1.032\n",
       "2  1.032  0.959  0.381 -0.192 -2.624  1.4112 -0.623\n",
       "3 -0.623  1.032  0.959  0.381 -0.192  1.2760  0.614\n",
       "4  0.614 -0.623  1.032  0.959  0.381  1.2057  0.213"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.astype(float) ## sometime need to convert to float\n",
    "X = X.astype(float) ## so that statsmodel functions can handle\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Binomial in module statsmodels.genmod.families.family:\n",
      "\n",
      "class Binomial(Family)\n",
      " |  Binomial(link=None)\n",
      " |  \n",
      " |  Binomial exponential family distribution.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  link : a link instance, optional\n",
      " |      The default link for the Binomial family is the logit link.\n",
      " |      Available links are logit, probit, cauchy, log, and cloglog.\n",
      " |      See statsmodels.genmod.families.links for more information.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  Binomial.link : a link instance\n",
      " |      The link function of the Binomial instance\n",
      " |  Binomial.variance : varfunc instance\n",
      " |      ``variance`` is an instance of\n",
      " |      statsmodels.genmod.families.varfuncs.binary\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  statsmodels.genmod.families.family.Family : Parent class for all links.\n",
      " |  :ref:`links` : Further details on links.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  endog for Binomial can be specified in one of three ways:\n",
      " |  A 1d array of 0 or 1 values, indicating failure or success\n",
      " |  respectively.\n",
      " |  A 2d array, with two columns. The first column represents the\n",
      " |  success count and the second column represents the failure\n",
      " |  count.\n",
      " |  A 1d array of proportions, indicating the proportion of\n",
      " |  successes, with parameter `var_weights` containing the\n",
      " |  number of trials for each row.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Binomial\n",
      " |      Family\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, link=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  initialize(self, endog, freq_weights)\n",
      " |      Initialize the response variable.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      endog : ndarray\n",
      " |          Endogenous response variable\n",
      " |      freq_weights : ndarray\n",
      " |          1d array of frequency weights\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      If `endog` is binary, returns `endog`\n",
      " |      \n",
      " |      If `endog` is a 2d array, then the input is assumed to be in the format\n",
      " |      (successes, failures) and\n",
      " |      successes/(success + failures) is returned.  And n is set to\n",
      " |      successes + failures.\n",
      " |  \n",
      " |  loglike_obs(self, endog, mu, var_weights=1.0, scale=1.0)\n",
      " |      The log-likelihood function for each observation in terms of the fitted\n",
      " |      mean response for the Binomial distribution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      endog : ndarray\n",
      " |          Usually the endogenous response variable.\n",
      " |      mu : ndarray\n",
      " |          Usually but not always the fitted mean response variable.\n",
      " |      var_weights : array_like\n",
      " |          1d array of variance (analytic) weights. The default is 1.\n",
      " |      scale : float\n",
      " |          The scale parameter. The default is 1.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ll_i : float\n",
      " |          The value of the loglikelihood evaluated at\n",
      " |          (endog, mu, var_weights, scale) as defined below.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If the endogenous variable is binary:\n",
      " |      \n",
      " |      .. math::\n",
      " |      \n",
      " |       ll_i = \\sum_i (y_i * \\log(\\mu_i/(1-\\mu_i)) + \\log(1-\\mu_i)) *\n",
      " |             var\\_weights_i\n",
      " |      \n",
      " |      If the endogenous variable is binomial:\n",
      " |      \n",
      " |      .. math::\n",
      " |      \n",
      " |         ll_i = \\sum_i var\\_weights_i * (\\ln \\Gamma(n+1) -\n",
      " |                \\ln \\Gamma(y_i + 1) - \\ln \\Gamma(n_i - y_i +1) + y_i *\n",
      " |                \\log(\\mu_i / (n_i - \\mu_i)) + n * \\log(1 - \\mu_i/n_i))\n",
      " |      \n",
      " |      where :math:`y_i = Y_i * n_i` with :math:`Y_i` and :math:`n_i` as\n",
      " |      defined in Binomial initialize.  This simply makes :math:`y_i` the\n",
      " |      original number of successes.\n",
      " |  \n",
      " |  resid_anscombe(self, endog, mu, var_weights=1.0, scale=1.0)\n",
      " |      The Anscombe residuals\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      endog : ndarray\n",
      " |          The endogenous response variable\n",
      " |      mu : ndarray\n",
      " |          The inverse of the link function at the linear predicted values.\n",
      " |      var_weights : array_like\n",
      " |          1d array of variance (analytic) weights. The default is 1.\n",
      " |      scale : float, optional\n",
      " |          An optional argument to divide the residuals by sqrt(scale).\n",
      " |          The default is 1.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      resid_anscombe : ndarray\n",
      " |          The Anscombe residuals as defined below.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. math::\n",
      " |      \n",
      " |          n^{2/3}*(cox\\_snell(endog)-cox\\_snell(mu)) /\n",
      " |          (mu*(1-mu/n)*scale^3)^{1/6} * \\sqrt(var\\_weights)\n",
      " |      \n",
      " |      where cox_snell is defined as\n",
      " |      cox_snell(x) = betainc(2/3., 2/3., x)*betainc(2/3.,2/3.)\n",
      " |      where betainc is the incomplete beta function as defined in scipy,\n",
      " |      which uses a regularized version (with the unregularized version, one\n",
      " |      would just have :math:`cox_snell(x) = Betainc(2/3., 2/3., x)`).\n",
      " |      \n",
      " |      The name 'cox_snell' is idiosyncratic and is simply used for\n",
      " |      convenience following the approach suggested in Cox and Snell (1968).\n",
      " |      Further note that\n",
      " |      :math:`cox\\_snell(x) = \\frac{3}{2}*x^{2/3} *\n",
      " |      hyp2f1(2/3.,1/3.,5/3.,x)`\n",
      " |      where hyp2f1 is the hypergeometric 2f1 function.  The Anscombe\n",
      " |      residuals are sometimes defined in the literature using the\n",
      " |      hyp2f1 formulation.  Both betainc and hyp2f1 can be found in scipy.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      Anscombe, FJ. (1953) \"Contribution to the discussion of H. Hotelling's\n",
      " |          paper.\" Journal of the Royal Statistical Society B. 15, 229-30.\n",
      " |      \n",
      " |      Cox, DR and Snell, EJ. (1968) \"A General Definition of Residuals.\"\n",
      " |          Journal of the Royal Statistical Society B. 30, 248-75.\n",
      " |  \n",
      " |  starting_mu(self, y)\n",
      " |      The starting values for the IRLS algorithm for the Binomial family.\n",
      " |      A good choice for the binomial family is :math:`\\mu_0 = (Y_i + 0.5)/2`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  links = [<class 'statsmodels.genmod.families.links.logit'>, <class 'st...\n",
      " |  \n",
      " |  safe_links = [<class 'statsmodels.genmod.families.links.Logit'>, <clas...\n",
      " |  \n",
      " |  variance = <statsmodels.genmod.families.varfuncs.Binomial object>\n",
      " |      The binomial variance function for n = 1\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is an alias of Binomial(n=1)\n",
      " |  \n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Family:\n",
      " |  \n",
      " |  deviance(self, endog, mu, var_weights=1.0, freq_weights=1.0, scale=1.0)\n",
      " |      The deviance function evaluated at (endog, mu, var_weights,\n",
      " |      freq_weights, scale) for the distribution.\n",
      " |      \n",
      " |      Deviance is usually defined as twice the loglikelihood ratio.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      endog : array_like\n",
      " |          The endogenous response variable\n",
      " |      mu : array_like\n",
      " |          The inverse of the link function at the linear predicted values.\n",
      " |      var_weights : array_like\n",
      " |          1d array of variance (analytic) weights. The default is 1.\n",
      " |      freq_weights : array_like\n",
      " |          1d array of frequency weights. The default is 1.\n",
      " |      scale : float, optional\n",
      " |          An optional scale argument. The default is 1.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Deviance : ndarray\n",
      " |          The value of deviance function defined below.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Deviance is defined\n",
      " |      \n",
      " |      .. math::\n",
      " |      \n",
      " |         D = 2\\sum_i (freq\\_weights_i * var\\_weights *\n",
      " |         (llf(endog_i, endog_i) - llf(endog_i, \\mu_i)))\n",
      " |      \n",
      " |      where y is the endogenous variable. The deviance functions are\n",
      " |      analytically defined for each family.\n",
      " |      \n",
      " |      Internally, we calculate deviance as:\n",
      " |      \n",
      " |      .. math::\n",
      " |         D = \\sum_i freq\\_weights_i * var\\_weights * resid\\_dev_i  / scale\n",
      " |  \n",
      " |  fitted(self, lin_pred)\n",
      " |      Fitted values based on linear predictors lin_pred.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      lin_pred : ndarray\n",
      " |          Values of the linear predictor of the model.\n",
      " |          :math:`X \\cdot \\beta` in a classical linear model.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      mu : ndarray\n",
      " |          The mean response variables given by the inverse of the link\n",
      " |          function.\n",
      " |  \n",
      " |  loglike(self, endog, mu, var_weights=1.0, freq_weights=1.0, scale=1.0)\n",
      " |      The log-likelihood function in terms of the fitted mean response.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      endog : ndarray\n",
      " |          Usually the endogenous response variable.\n",
      " |      mu : ndarray\n",
      " |          Usually but not always the fitted mean response variable.\n",
      " |      var_weights : array_like\n",
      " |          1d array of variance (analytic) weights. The default is 1.\n",
      " |      freq_weights : array_like\n",
      " |          1d array of frequency weights. The default is 1.\n",
      " |      scale : float\n",
      " |          The scale parameter. The default is 1.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ll : float\n",
      " |          The value of the loglikelihood evaluated at\n",
      " |          (endog, mu, var_weights, freq_weights, scale) as defined below.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Where :math:`ll_i` is the by-observation log-likelihood:\n",
      " |      \n",
      " |      .. math::\n",
      " |         ll = \\sum(ll_i * freq\\_weights_i)\n",
      " |      \n",
      " |      ``ll_i`` is defined for each family. endog and mu are not restricted\n",
      " |      to ``endog`` and ``mu`` respectively.  For instance, you could call\n",
      " |      both ``loglike(endog, endog)`` and ``loglike(endog, mu)`` to get the\n",
      " |      log-likelihood ratio.\n",
      " |  \n",
      " |  predict(self, mu)\n",
      " |      Linear predictors based on given mu values.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      mu : ndarray\n",
      " |          The mean response variables\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      lin_pred : ndarray\n",
      " |          Linear predictors based on the mean response variables.  The value\n",
      " |          of the link function at the given mu.\n",
      " |  \n",
      " |  resid_dev(self, endog, mu, var_weights=1.0, scale=1.0)\n",
      " |      The deviance residuals\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      endog : array_like\n",
      " |          The endogenous response variable\n",
      " |      mu : array_like\n",
      " |          The inverse of the link function at the linear predicted values.\n",
      " |      var_weights : array_like\n",
      " |          1d array of variance (analytic) weights. The default is 1.\n",
      " |      scale : float, optional\n",
      " |          An optional scale argument. The default is 1.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      resid_dev : float\n",
      " |          Deviance residuals as defined below.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The deviance residuals are defined by the contribution D_i of\n",
      " |      observation i to the deviance as\n",
      " |      \n",
      " |      .. math::\n",
      " |         resid\\_dev_i = sign(y_i-\\mu_i) \\sqrt{D_i}\n",
      " |      \n",
      " |      D_i is calculated from the _resid_dev method in each family.\n",
      " |      Distribution-specific documentation of the calculation is available\n",
      " |      there.\n",
      " |  \n",
      " |  weights(self, mu)\n",
      " |      Weights for IRLS steps\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      mu : array_like\n",
      " |          The transformed mean response variable in the exponential family\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      w : ndarray\n",
      " |          The weights for the IRLS steps\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. math::\n",
      " |      \n",
      " |         w = 1 / (g'(\\mu)^2  * Var(\\mu))\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Family:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  link\n",
      " |      Link function for family\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from Family:\n",
      " |  \n",
      " |  valid = [-inf, inf]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sm.families.Binomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.000001\n",
      "         Iterations: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajimuddin/opt/anaconda3/lib/python3.9/site-packages/statsmodels/discrete/discrete_model.py:1810: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "/Users/ajimuddin/opt/anaconda3/lib/python3.9/site-packages/statsmodels/base/model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "regr = sm.Logit(y, X, family = sm.families.Binomial()).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Results: Logit\n",
      "=================================================================\n",
      "Model:              Logit            Pseudo R-squared: 1.000     \n",
      "Dependent Variable: Direction        AIC:              14.0031   \n",
      "Date:               2022-09-12 23:39 BIC:              49.9193   \n",
      "No. Observations:   1250             Log-Likelihood:   -0.0015287\n",
      "Df Model:           6                LL-Null:          -865.59   \n",
      "Df Residuals:       1243             LLR p-value:      0.0000    \n",
      "Converged:          0.0000           Scale:            1.0000    \n",
      "No. Iterations:     35.0000                                      \n",
      "-----------------------------------------------------------------\n",
      "          Coef.    Std.Err.    z    P>|z|     [0.025     0.975]  \n",
      "-----------------------------------------------------------------\n",
      "Lag1       1.8453   47.0313  0.0392 0.9687    -90.3343    94.0248\n",
      "Lag2       4.0292  112.6450  0.0358 0.9715   -216.7510   224.8094\n",
      "Lag3      -2.4198  139.6977 -0.0173 0.9862   -276.2222   271.3826\n",
      "Lag4      -2.4258  109.2056 -0.0222 0.9823   -216.4647   211.6132\n",
      "Lag5       6.4171   73.9616  0.0868 0.9309   -138.5449   151.3791\n",
      "Volume     9.0200   65.7794  0.1371 0.8909   -119.9052   137.9452\n",
      "Today   3103.6405 9663.6946  0.3212 0.7481 -15836.8527 22044.1338\n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(regr.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smallest p-value here is associated with Lag1. The negative coefficient for this predictor suggests that if the market had a positive return yesterday, then it is less likely to go up today. However, at a value of 0.15, the p-value is still relatively large, and so there is no clear evidence of a real association between Lag1 and Direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag1         1.845263\n",
      "Lag2         4.029186\n",
      "Lag3        -2.419767\n",
      "Lag4        -2.425797\n",
      "Lag5         6.417094\n",
      "Volume       9.020000\n",
      "Today     3103.640533\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(regr.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.0\n",
      "1    1.0\n",
      "2    0.0\n",
      "3    1.0\n",
      "4    1.0\n",
      "5    1.0\n",
      "6    0.0\n",
      "7    1.0\n",
      "8    1.0\n",
      "9    1.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajimuddin/opt/anaconda3/lib/python3.9/site-packages/statsmodels/discrete/discrete_model.py:1810: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    }
   ],
   "source": [
    "pred = regr.predict(X)\n",
    "print(pred[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict() function can be used to predict the probability that the market will go up, given values of the predictors. The type=\"response\" option tells R to output probabilities of the form P(Y = 1|X), as opposed to other information such as the logit. If no data set is supplied to the predict() function, then the probabilities are computed for the training data that was used to fit the logistic regression model. Here we have printed only the first ten probabilities. We know that these values correspond to the probability of the market going up, rather than down.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast(variable):\n",
    "    level = list(set(variable))\n",
    "    n = len(level)\n",
    "    df = pd.DataFrame(np.zeros([n,n]), index = level, columns = level)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            df.iloc[i,j] = 1 if i==j else 0\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Down   Up\n",
      "Down   1.0  0.0\n",
      "Up     0.0  1.0\n"
     ]
    }
   ],
   "source": [
    "print(contrast(data.Direction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[602   0]\n",
      " [  0 648]]\n"
     ]
    }
   ],
   "source": [
    "result = ['Up' if r > .5 else 'Down' for r in pred]\n",
    "print(confusion_matrix(y_pred = result, y_true = data.Direction))  # sklearn version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function confusion_matrix in module sklearn.metrics._classification:\n",
      "\n",
      "confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)\n",
      "    Compute confusion matrix to evaluate the accuracy of a classification.\n",
      "    \n",
      "    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\n",
      "    is equal to the number of observations known to be in group :math:`i` and\n",
      "    predicted to be in group :math:`j`.\n",
      "    \n",
      "    Thus in binary classification, the count of true negatives is\n",
      "    :math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is\n",
      "    :math:`C_{1,1}` and false positives is :math:`C_{0,1}`.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true : array-like of shape (n_samples,)\n",
      "        Ground truth (correct) target values.\n",
      "    \n",
      "    y_pred : array-like of shape (n_samples,)\n",
      "        Estimated targets as returned by a classifier.\n",
      "    \n",
      "    labels : array-like of shape (n_classes), default=None\n",
      "        List of labels to index the matrix. This may be used to reorder\n",
      "        or select a subset of labels.\n",
      "        If ``None`` is given, those that appear at least once\n",
      "        in ``y_true`` or ``y_pred`` are used in sorted order.\n",
      "    \n",
      "    sample_weight : array-like of shape (n_samples,), default=None\n",
      "        Sample weights.\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "    \n",
      "    normalize : {'true', 'pred', 'all'}, default=None\n",
      "        Normalizes confusion matrix over the true (rows), predicted (columns)\n",
      "        conditions or all the population. If None, confusion matrix will not be\n",
      "        normalized.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    C : ndarray of shape (n_classes, n_classes)\n",
      "        Confusion matrix whose i-th row and j-th\n",
      "        column entry indicates the number of\n",
      "        samples with true label being i-th class\n",
      "        and predicted label being j-th class.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    ConfusionMatrixDisplay.from_estimator : Plot the confusion matrix\n",
      "        given an estimator, the data, and the label.\n",
      "    ConfusionMatrixDisplay.from_predictions : Plot the confusion matrix\n",
      "        given the true and predicted labels.\n",
      "    ConfusionMatrixDisplay : Confusion Matrix visualization.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] `Wikipedia entry for the Confusion matrix\n",
      "           <https://en.wikipedia.org/wiki/Confusion_matrix>`_\n",
      "           (Wikipedia and other references may use a different\n",
      "           convention for axes).\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.metrics import confusion_matrix\n",
      "    >>> y_true = [2, 0, 2, 2, 0, 1]\n",
      "    >>> y_pred = [0, 0, 2, 2, 0, 2]\n",
      "    >>> confusion_matrix(y_true, y_pred)\n",
      "    array([[2, 0, 0],\n",
      "           [0, 0, 1],\n",
      "           [1, 0, 2]])\n",
      "    \n",
      "    >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "    >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "    >>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\n",
      "    array([[2, 0, 0],\n",
      "           [0, 0, 1],\n",
      "           [1, 0, 2]])\n",
      "    \n",
      "    In the binary case, we can extract true positives, etc as follows:\n",
      "    \n",
      "    >>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
      "    >>> (tn, fp, fn, tp)\n",
      "    (0, 2, 1, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonal elements of the confusion matrix indicate correct predictions, while the off-diagonals represent incorrect predictions. Hence our model correctly predicted that the market would go up on 507 days and that it would go down on 145 days, for a total of 507 + 145 = 652 correct predictions. The mean() function can be used to compute the fraction of days for which the prediction was correct. In this case, logistic regression correctly predicted the movement of the market 52.2 % of the time.\n",
    "At first glance, it appears that the logistic regression model is working a little better than random guessing. However, this result is misleading because we trained and tested the model on the same set of 1, 250 observa- tions. In other words, 100 − 52.2 = 47.8 % is the training error rate. As we have seen previously, the training error rate is often overly optimistic, it tends to underestimate the test error rate. In order to better assess the ac- curacy of the logistic regression model in this setting, we can fit the model using part of the data, and then examine how well it predicts the held out data. This will yield a more realistic error rate, in the sense that in practice we will be interested in our model’s performance not on the data that we used to fit the model, but rather on days in the future for which the market’s movements are unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_mat(y_true, y_pred): # function version\n",
    "    df = contrast(y_true) # columns true, rows pred\n",
    "    name = df.columns.tolist()\n",
    "    n = len(y_pred)\n",
    "    for j in range(len(name)):\n",
    "        df.iloc[j,j] = 0\n",
    "    for i in range(n):\n",
    "        if y_true.iloc[i] == name[0]:\n",
    "            if y_pred[i] == name[0]:\n",
    "                df.iloc[0,0] += 1\n",
    "            else :\n",
    "                df.iloc[1,0] += 1\n",
    "        else :\n",
    "            if y_pred[i] == name[0]:\n",
    "                df.iloc[0,1] += 1\n",
    "            else :\n",
    "                df.iloc[1,1] += 1\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Down     Up\n",
      "Down  602.0    0.0\n",
      "Up      0.0  648.0 \n",
      "\n",
      "1.0 \n",
      "\n",
      "0.5184001090493044\n"
     ]
    }
   ],
   "source": [
    "conf = conf_mat(data.Direction, result)\n",
    "print(conf, '\\n')\n",
    "print((conf.iloc[0,0]+conf.iloc[1,1])/len(y), '\\n')\n",
    "print(np.mean(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data.Year < 2005] # train-test split\n",
    "test = data[[not a for a in data.Year < 2005]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.iloc[:,1:7].copy()\n",
    "y = train.iloc[:,-1].copy()\n",
    "\n",
    "X_test = test.iloc[:,1:7].copy()\n",
    "y_test = test.iloc[:,-1].copy()\n",
    "\n",
    "n = len(y)\n",
    "for i in range(n):\n",
    "    if y[i] == 'Up':\n",
    "        y[i] = 1\n",
    "    elif y[i] == 'Down':\n",
    "        y[i] = 0\n",
    "\n",
    "y = y.astype(float)\n",
    "X = X.astype(float)\n",
    "X_test = X_test.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object train is a vector of 1,250 elements, corresponding to the ob- servations in our data set. The elements of the vector that correspond to observations that occurred before 2005 are set to TRUE, whereas those that correspond to observations in 2005 are set to FALSE. The object train is a Boolean vector, since its elements are TRUE and FALSE. Boolean vectors can be used to obtain a subset of the rows or columns of a matrix. For instance, the command Smarket[train,] would pick out a submatrix of the stock market data set, corresponding only to the dates before 2005, since those are the ones for which the elements of train are TRUE. The ! symbol can be used to reverse all of the elements of a Boolean vector. That is, !train is a vector similar to train, except that the elements that are TRUE in train get swapped to FALSE in !train, and the elements that are FALSE in train get swapped to TRUE in !train. Therefore, Smarket[!train,] yields a submatrix of the stock market data containing only the observations for which train is FALSE—that is, the observations with dates in 2005. The output above indicates that there are 252 such observations.\n",
    "We now fit a logistic regression model using only the subset of the obser- vations that correspond to dates before 2005, using the subset argument. We then obtain predicted probabilities of the stock market going up for each of the days in our test set—that is, for the days in 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692101\n",
      "         Iterations 3\n",
      "                         Results: Logit\n",
      "================================================================\n",
      "Model:              Logit            Pseudo R-squared: 0.001    \n",
      "Dependent Variable: Direction        AIC:              1393.4337\n",
      "Date:               2022-09-12 23:39 BIC:              1422.8682\n",
      "No. Observations:   998              Log-Likelihood:   -690.72  \n",
      "Df Model:           5                LL-Null:          -691.63  \n",
      "Df Residuals:       992              LLR p-value:      0.87193  \n",
      "Converged:          1.0000           Scale:            1.0000   \n",
      "No. Iterations:     3.0000                                      \n",
      "------------------------------------------------------------------\n",
      "           Coef.    Std.Err.      z      P>|z|     [0.025   0.975]\n",
      "------------------------------------------------------------------\n",
      "Lag1      -0.0557     0.0517   -1.0775   0.2812   -0.1571   0.0456\n",
      "Lag2      -0.0440     0.0517   -0.8506   0.3950   -0.1453   0.0573\n",
      "Lag3       0.0092     0.0515    0.1783   0.8585   -0.0918   0.1102\n",
      "Lag4       0.0089     0.0515    0.1723   0.8632   -0.0921   0.1099\n",
      "Lag5      -0.0031     0.0511   -0.0602   0.9520   -0.1032   0.0971\n",
      "Volume     0.0186     0.0455    0.4077   0.6835   -0.0706   0.1078\n",
      "================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regr = sm.Logit(y, X, family=sm.families.Binomial()).fit()\n",
    "print(regr.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5085663310527532\n"
     ]
    }
   ],
   "source": [
    "pred = regr.predict(X_test)\n",
    "result = ['Up' if r > .5 else 'Down' for r in pred]\n",
    "print(np.mean(pred)) ## out-of-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Down     Up\n",
      "Down  33.0   26.0\n",
      "Up    78.0  115.0 \n",
      "\n",
      "0.5873015873015873\n"
     ]
    }
   ],
   "source": [
    "conf = conf_mat(y_test, result)\n",
    "print(conf, '\\n')\n",
    "print((conf.iloc[0,0]+conf.iloc[1,1])/np.sum(conf.values)) ## out-of-sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the results appear to be a little better: 58.7% of the daily movements have been correctly predicted. It is worth noting that in this case, a much simpler strategy of predicting that the market will increase every day will also be correct 56% of the time! Hence, in terms of overall error rate, the logistic regression method is no better than the naive approach. However, the confusion matrix shows that on days when logistic regression predicts an increase in the market, it has a 58% accuracy rate. This suggests a possible trading strategy of buying on days when the model predicts an increasing market, and avoiding trades on days when a decrease is predicted. Of course one would need to investigate more carefully whether this small improvement was real or just due to random chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692215\n",
      "         Iterations 3\n",
      "                         Results: Logit\n",
      "================================================================\n",
      "Model:              Logit            Pseudo R-squared: 0.001    \n",
      "Dependent Variable: Direction        AIC:              1385.6605\n",
      "Date:               2022-09-12 23:39 BIC:              1395.4720\n",
      "No. Observations:   998              Log-Likelihood:   -690.83  \n",
      "Df Model:           1                LL-Null:          -691.63  \n",
      "Df Residuals:       996              LLR p-value:      0.20523  \n",
      "Converged:          1.0000           Scale:            1.0000   \n",
      "No. Iterations:     3.0000                                      \n",
      "------------------------------------------------------------------\n",
      "           Coef.    Std.Err.      z      P>|z|     [0.025   0.975]\n",
      "------------------------------------------------------------------\n",
      "Lag1      -0.0556     0.0517   -1.0751   0.2823   -0.1569   0.0457\n",
      "Lag2      -0.0445     0.0517   -0.8606   0.3895   -0.1457   0.0568\n",
      "================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = train.iloc[:,1:3].copy()\n",
    "y = train.iloc[:,-1].copy()\n",
    "\n",
    "X_test = test.iloc[:,1:3].copy()\n",
    "y_test = test.iloc[:,-1].copy()\n",
    "\n",
    "n = len(y)\n",
    "for i in range(n):\n",
    "    if y[i] == 'Up':\n",
    "        y[i] = 1\n",
    "    elif y[i] == 'Down':\n",
    "        y[i] = 0\n",
    "\n",
    "y = y.astype(float)\n",
    "X = X.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "\n",
    "regr = sm.Logit(y, X, family=sm.families.Binomial()).fit()\n",
    "print(regr.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49960585327087725\n"
     ]
    }
   ],
   "source": [
    "pred = regr.predict(X_test)\n",
    "result = ['Up' if r > .5 else 'Down' for r in pred]\n",
    "print(np.mean(pred)) ## out-of-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Down    Up\n",
      "Down  64.0  67.0\n",
      "Up    47.0  74.0 \n",
      "\n",
      "0.5476190476190477\n"
     ]
    }
   ],
   "source": [
    "conf = conf_mat(y_test, result)\n",
    "print(conf, '\\n')\n",
    "print((conf.iloc[0,0]+conf.iloc[1,1])/np.sum(conf.values)) ## out-of-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4667031 , 0.49360611])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[1.2,1.5],[1.1,-0.8]])\n",
    "regr.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.3 Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will perform LDA on the Smarket data. In R, we fit an LDA model using the LinearDiscriminantAnalysis() function, which is part of the MASS library. Notice that the syntax for the LinearDiscriminantAnalysis() function is identical to that of logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49198397 0.50801603] \n",
      "\n",
      "[[ 0.04279022  0.03389409]\n",
      " [-0.03954635 -0.03132544]] \n",
      "\n",
      "[[-0.05544078 -0.0443452 ]] \n",
      "\n",
      "[0.03221375]\n"
     ]
    }
   ],
   "source": [
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X, y)\n",
    "print(clf.priors_, '\\n') # hint: tap 'tab' after dot '.' to see availible attributes of an obj\n",
    "print(clf.means_, '\\n')\n",
    "print(clf.coef_, '\\n')\n",
    "print(clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LDA output indicates that $\\hat{π}_1$ = 0.492 and $\\hat{π}_2 = 0.508$; in other words, 49.2% of the training observations correspond to days during which the market went down. It also provides the group means; these are the average of each predictor within each class, and are used by LDA as estimates of μk. These suggest that there is a tendency for the previous 2 days’ returns to be negative on days when the market increases, and a tendency for the previous days’ returns to be positive on days when the market declines. The coefficients of linear discriminants output provides the linear combination of Lag1 and Lag2 that are used to form the LDA decision rule. In other words, these are the multipliers of the elements of X = x in (4.19). If −0.642 × Lag1 − 0.514 × Lag2 is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a market decline. The plot() function produces plots of the linear discriminants, obtained by computing −0.642 × Lag1 − 0.514 × Lag2 for each of the training observations.\n",
    "\n",
    "The predict() function returns a list with three elements. The first ele- ment, class, contains LDA’s predictions about the movement of the market. The second element, posterior, is a matrix whose kth column contains the posterior probability that the corresponding observation belongs to the kth class, computed from (4.10). Finally, x contains the linear discriminants, described earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(X)\n",
    "x1 = np.arange(min(X.Lag1), max(X.Lag1))\n",
    "x2 = np.arange(min(X.Lag2), max(X.Lag2))\n",
    "yy = clf.intercept_ + x1 * clf.coef_[0,0] + x2 * clf.coef_[0,1]\n",
    "color = ['red' if a == 1 else 'green' for a in y]\n",
    "\n",
    "plt.scatter(X['Lag1'], X['Lag2'], s = 1, color = color)\n",
    "plt.plot(x1, yy, linewidth = 1, color = 'black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(X_test)\n",
    "print(clf.classes_, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Down     Up\n",
      "Down  35.0   35.0\n",
      "Up    76.0  106.0 \n",
      "\n",
      "0.5595238095238095\n"
     ]
    }
   ],
   "source": [
    "result = ['Up' if r > .5 else 'Down' for r in pred]\n",
    "conf = conf_mat(y_test, result)\n",
    "print(conf, '\\n')\n",
    "print((conf.iloc[0,0]+conf.iloc[1,1])/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "182\n"
     ]
    }
   ],
   "source": [
    "post = clf.predict_proba(X_test)\n",
    "#print(post, '\\n') # posteriors\n",
    "print(sum(post[:,0] >= .5))\n",
    "print(sum(post[:,0] < .5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49017925 0.4792185  0.46681848 0.47400107 0.49278766 0.49385615\n",
      " 0.49510156 0.4872861  0.49070135 0.48440262 0.49069628 0.51199885\n",
      " 0.48951523 0.47067612 0.47445929 0.47995834 0.49357753 0.50308938\n",
      " 0.49788061 0.48863309] \n",
      "\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1.] \n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(post[0:20,0], '\\n')\n",
    "print(pred[0:20], '\\n')\n",
    "print(sum(post[:,0] > .9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.4 Quadratic Discriminant Analysis\n",
    "\n",
    "We will now fit a QDA model to the Smarket data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49198397 0.50801603] \n",
      "\n",
      "[[ 0.04279022  0.03389409]\n",
      " [-0.03954635 -0.03132544]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = QuadraticDiscriminantAnalysis()\n",
    "clf.fit(X, y)\n",
    "print(clf.priors_, '\\n') # hint: tap 'tab' after dot '.' to see availible attributes of an obj\n",
    "print(clf.means_, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Down     Up\n",
      "Down  30.0   20.0\n",
      "Up    81.0  121.0 \n",
      "\n",
      "0.5992063492063492\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(X_test)\n",
    "result = ['Up' if r > .5 else 'Down' for r in pred]\n",
    "conf = conf_mat(y_test, result)\n",
    "print(conf, '\\n')\n",
    "print((conf.iloc[0,0]+conf.iloc[1,1])/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the QDA predictions are accurate almost 60% of the time, even though the 2005 data was not used to fit the model. This level of accuracy is quite impressive for stock market data, which is known to be quite hard to model accurately. This suggests that the quadratic form assumed by QDA may capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression. However, we recommend evaluating this method’s performance on a larger test set before betting that this approach will consistently beat the market!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform KNN using the KNeighborsClassifier() function. \n",
    "This function works rather differently from the other model fitting functions that we have encountered thus far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Down    Up\n",
      "Down  43.0  58.0\n",
      "Up    68.0  83.0 \n",
      "\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors = 1)\n",
    "clf.fit(X, y)\n",
    "pred = clf.predict(X_test)\n",
    "result = ['Up' if r > .5 else 'Down' for r in pred]\n",
    "conf = conf_mat(y_test, result)\n",
    "print(conf, '\\n')\n",
    "print((conf.iloc[0,0]+conf.iloc[1,1])/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Down    Up\n",
      "Down  48.0  55.0\n",
      "Up    63.0  86.0 \n",
      "\n",
      "0.5317460317460317\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors = 3)\n",
    "clf.fit(X, y)\n",
    "pred = clf.predict(X_test)\n",
    "result = ['Up' if r > .5 else 'Down' for r in pred]\n",
    "conf = conf_mat(y_test, result)\n",
    "print(conf, '\\n')\n",
    "print((conf.iloc[0,0]+conf.iloc[1,1])/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.6 An Application to Caravan Insuarance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5822, 87) \n",
      "\n",
      "count     5822\n",
      "unique       2\n",
      "top         No\n",
      "freq      5474\n",
      "Name: Purchase, dtype: object \n",
      "\n",
      "348 5474 \n",
      "\n",
      "0.05977327378907592\n"
     ]
    }
   ],
   "source": [
    "Caravan = pd.read_table(\"Data/Caravan.csv\", sep = ',')\n",
    "print(Caravan.shape, '\\n')\n",
    "print(Caravan.Purchase.describe(), '\\n')\n",
    "print(sum(Caravan.Purchase == 'Yes'), sum(Caravan.Purchase == 'No'), '\\n')\n",
    "print(sum(Caravan.Purchase == 'Yes')/len(Caravan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1106149089659911\n",
      "-1.6140377328112892e-16 \n",
      "\n",
      "0.1646794913476145\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "X = Caravan.drop('Purchase', axis = 1)\n",
    "X = preprocessing.scale(X, axis = 0)\n",
    "print(np.mean(Caravan.iloc[:,i]))\n",
    "print(np.mean(X[:,i]), '\\n')\n",
    "print(np.var(Caravan.iloc[:,i]))\n",
    "print(np.var(X[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list(range(1000))\n",
    "train = list(range(1000, len(Caravan)))\n",
    "X_train = X[train]\n",
    "X_test = X[test]\n",
    "y_train = Caravan.Purchase[train]\n",
    "y_test = Caravan.Purchase[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors = 1)\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Yes     No\n",
      "Yes  11.0   59.0\n",
      "No   48.0  882.0 \n",
      "\n",
      "0.9483870967741935\n"
     ]
    }
   ],
   "source": [
    "conf = conf_mat(y_test, pred)\n",
    "print(conf, '\\n')\n",
    "print(conf.iloc[1,1]/(conf.iloc[1,0]+conf.iloc[1,1])) # don't know sometime it changes. [Yes,Yes]/([Yes,Yes]+[No,Yes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Yes     No\n",
      "Yes   6.0   20.0\n",
      "No   53.0  921.0 \n",
      "\n",
      "0.9455852156057495\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors = 3)\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "conf = conf_mat(y_test, pred)\n",
    "print(conf, '\\n')\n",
    "print(conf.iloc[1,1]/(conf.iloc[1,0]+conf.iloc[1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Yes     No\n",
      "Yes   4.0    7.0\n",
      "No   55.0  934.0 \n",
      "\n",
      "0.9443882709807887\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors = 5)\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "conf = conf_mat(y_test, pred)\n",
    "print(conf, '\\n')\n",
    "print(conf.iloc[1,1]/(conf.iloc[1,0]+conf.iloc[1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_num = [1 if a == 'Yes' else 0 for a in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.634868\n",
      "         Iterations 6\n",
      "      Yes     No\n",
      "Yes  59.0  941.0\n",
      "No    0.0    0.0 \n",
      "\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nc/ypgpz5s910qf6m8s4fpj_lp40000gn/T/ipykernel_18633/1773162225.py:6: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  print(conf.iloc[1,1]/(conf.iloc[1,0]+conf.iloc[1,1]))\n"
     ]
    }
   ],
   "source": [
    "regr = sm.Logit(y_train_num, X_train, family=sm.families.Binomial()).fit()\n",
    "pred = regr.predict(X_test)\n",
    "result = ['Yes' if r > .5 else 'No' for r in pred]\n",
    "conf = conf_mat(y_test, result)\n",
    "print(conf, '\\n')\n",
    "print(conf.iloc[1,1]/(conf.iloc[1,0]+conf.iloc[1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Yes     No\n",
      "Yes   5.0   10.0\n",
      "No   54.0  931.0 \n",
      "\n",
      "0.9451776649746193\n"
     ]
    }
   ],
   "source": [
    "thr = 0.9\n",
    "result = ['Yes' if r > thr else 'No' for r in pred]\n",
    "conf = conf_mat(y_test, result)\n",
    "print(conf, '\\n')\n",
    "print(conf.iloc[1,1]/(conf.iloc[1,0]+conf.iloc[1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
